---
title: "rock model"
format: docx
editor: visual
---

## Rock Ptarmigan Distance Time Removal Model

```{r clean}
# Clean your workspace to reset your R environment
rm( list = ls() )
# Check that you are in the right project folder
getwd()
```

## Library

```{r library}
library(tidyr)
library(dplyr)
library(unmarked)
library(raster)
library(rgdal)
library(ggplot2)

```

## Load Data

```{r load}
rocks <- read.csv("E:/gyrf_analysis/MSgyrfalcon/MSgyrfalcon/rockmalesonly_timedist_9nov23.csv")
rocks <- rocks[,-c(1)]

# fix NA distance - it was listed in the distance category column in original data frame

rocks[30,3] <- 400

env.s <- read.csv("E:/gyrf_analysis/MSgyrfalcon/MSgyrfalcon/pland2_400r.csv")
env.s <- env.s[,-c(1)]
env.s[8,1] <- "COUN_3_12_4"

scaled <- read.csv("E:/gyrf_analysis/MSgyrfalcon/MSgyrfalcon/wipt_male_df_18Oct23.csv")
scaled <- scaled[,-c(1, 3:7)]

```

## Create unmarkedFrameGDR

### Distance Matrix

```{r dist matrix}

# create a separate df to manipulate for distance data
rocks2 <- data.frame(rocks)

# now i need to get exact distances to 'binned data'


my.bins <- seq(0, 400, by = 25)

rocks2$distance_bin <- cut(rocks2$exact_distance, breaks = my.bins, include.lowest = TRUE)

# add empty sites (sites that had no WIPT male observations)

# first figure out what sites are missing
obs.ids <- as.data.frame(rocks2$id)
all.ids <- as.data.frame(env.s$id)

missing_df <- as.data.frame(setdiff(all.ids$`env.s$id`, obs.ids$`rocks2$id`))
colnames(missing_df)[1] <- "id" # change the name of the first column to 'id'

# now need to create dataframe that attaches empty sites to end of sites where ptarmigan were observed so i can then match those 'levels' of sites to the data frame of only observed. It make sure the formatting code knows there are sites where ptarmigan weren't observed. But the empty sites need to be attached to the end of the observed sites in an actual dataframe and then those levels matched to the observed dataframe as 'ghost' levels or else the order will be messed up and it will put the 'observed' ptarmigan into sites that were actually empty.

# so say the observed dataframe is df.obs = 'a, b, a, c, c, d' and you also had a site 'e' that was empty that you want to add to the levels. so a dataframe with all sites/levels, df.allsites = 'a, b, c, d, e'

# if you were to try and match the levels like 'levels(df.obs$id) <- levels(df.allsites$id) it would change the order of the observed sites to 'a, b, c, d, e' so then e would be matched up with a row that has an observation. you need to create a df that matches the same pattern of repeated site observations then add all the missing ones at the end: df.match = 'a, b, a, c, c, d, e' so then when you match them up the first sequence of 'a, b, a, c, c, d' matches up, then e is put at the end with empty values


# create my empty data frame I'm going to attach

num_columns <- 4 # define number of columns
num_rows <- 945 #define number of rows(the number of empty sites I'm adding)
col_names <- colnames(rocks2) # define what column names will be (the same as the frame i'm binding too)

# create the dataframe wtih my specifications
my_df <- data.frame(matrix(0, nrow = num_rows, ncol = num_columns))

# and add column names
colnames(my_df) <- col_names

# and replace 'id' column with names of all empty sites
my_df$id <- missing_df$id

# need to 'add' missing factor levels to the dataframe I'll actually be putting into the formatting code (creates 'ghost' sites for all empty sites)


rocks2$id <- as.factor(rocks2$id)
my_df$id <- as.factor(my_df$id)

rocks2$id <- factor(rocks2$id, levels = union(levels(rocks2$id), levels(my_df$id)))
levels(rocks2$id) # should be 988

########### format distance data for frame########
#make my bin breaks into a list so i can just put the object into the formatting insteading having to list them all out
my.bin.list <- as.list(my.bins)

# format distance data for the unmarkedFrameGDR:
yDist <- formatDistData(rocks2, distCol = "exact_distance", transectNameCol = "id", dist.breaks = my.bin.list)


sum(yDist) # should be 55 and visually check that lenght is 988
```

### Time Matrix

```{r time matrix}
# can use same formatDistData function but replace distance with time-intervals

yRem <- formatDistData(rocks2, distCol = "time_int", transectNameCol = "id", dist.breaks = c(0, 2, 4, 6, 8, 10))

sum(yRem) # make sure its 55 and visually check that there are 988 rows/sites
```

## Build unmarkedFrameGDR

```{r frame}

# not sure if this makes a difference but put env covs in same order as yDist and yRem

df1 <- data.frame(yDist)
# Extract the row names of the matrix
row_names <- rownames(df1)

# Add the row names as a new column in the dataframe
df1$id <- row_names

# make sure both are factors
df1$id <- as.factor(df1$id)
scaled$id <- as.factor(scaled$id)

# join scaled environmental to dataframe that is in same order as dist and time matrices
covs_df <- full_join(df1, scaled, by = 'id')

# add area to covs data
# area of the survey area was a 400m radius circle
# standardize the value by dividing by 10000

covs_df$area <- pi*400*400/10000


# site covariates: id, low shrub, sparse veg, tundra, elevation, area
siCovs <- covs_df[,c(17, 23, 26, 28, 32, 46)]
siCovs$id <- as.factor(siCovs$id)

# in the frame these are actually the yearlySiteCovs: id, julian, min after sunrise, observer, wind speed
oCovs <- covs_df[,c(17, 33, 39, 43:44)]
oCovs$id <- as.factor(oCovs$id)
oCovs$observer <- as.factor(oCovs$observer)

breaks <- seq(0, 400, by = 25)





umfGDR <- unmarkedFrameGDR(yDistance = yDist, yRemoval = yRem, numPrimary = 1, siteCovs = siCovs, obsCovs = NULL, yearlySiteCovs = oCovs, dist.breaks = breaks, unitsIn = 'm', period.lengths = NULL)
```

## Distance Removal Model: Poisson vs ZIP

using function gdistremoval() in 'unmarked'

### Poisson

```{r pois}
# lambda:abundance, phi:availability (X - don't have these), removal:removal probablity covariates (these are my observer, date, etc), distance: detection function covariates

# data: object of class unmarkedFrameGDR
# keyfun: detection functions : "halfnorm", "hazard", "exp", or "uniform"
# output: "abund" or "density"
# unitsOut: units of density. either "ha" or "kmsq"
# mixture: either "P" "NB" or "ZIP"
# K: interger value speicying upper bound used in the integration
# starts: vector of starting values for model parameters
# methdod: optimization method used by optim
# se: logical specifying whether or not to comput standard errors
# engine:either "C" to use C++ code or "TMB" to use TMB for optimization
# threads: more stuff about C++... OpenMP


# my abundance covariates for wipt: tundra, tussock, tall shrub, elevation

um1.p <- gdistremoval(lambdaformula = ~1 + tundra + lowshrub + sparseveg + elev + offset(log(area)), phiformula = ~1, removalformula = ~1 + julian + min_after_sun + wind + observer, data = umfGDR, keyfun = "halfnorm", output = "density", unitsOut = "kmsq", mixture = "P")

summary(um1.p)
confint(um1.p, type = "lambda")
confint(um1.p, type = "rem")
```

### ZIP

```{r zip}

um1.zip <- gdistremoval(lambdaformula = ~1 + tundra + lowshrub + sparseveg + elev + offset(log(area)), phiformula = ~1, removalformula = ~1 + julian + min_after_sun + wind + observer, data = umfGDR, keyfun = "halfnorm", output = "density", unitsOut = "kmsq", mixture = "ZIP")

summary(um1.zip)
confint(um1.zip, type = "lambda")
confint(um1.zip, type = "rem")
```

## Spatial Predictions

Upload then standardize 1km resolution vegetation rasters.

```{r prediction}
# First need to scale study-wide rasters to match scaling of model:


elev <- raster("E:/gyrf_analysis/MSgyrfalcon/1k rasters/dem_1k.tif")

lowshrub <- raster("E:/gyrf_analysis/MSgyrfalcon/MSgyrfalcon/distance model/dist_data/lowshrub.tif")

tundra <- raster("E:/gyrf_analysis/MSgyrfalcon/MSgyrfalcon/distance model/dist_data/tundra.tif")

sparseveg <- raster("E:/gyrf_analysis/MSgyrfalcon/MSgyrfalcon/distance model/dist_data/sparseveg.tif")



# standardize by each rasters mean and standard deviation

# elev
cellStats(elev, mean) #173.7808
cellStats(elev, sd) #120.5669

elev.s <- (elev-173.7808)/120.5669

# tundra
cellStats(tundra, mean) #0.1608737
cellStats(tundra, sd) #0.08171017

tundra.s <- (tundra-0.1608737)/0.08171017

# lowshrub
cellStats(lowshrub, mean) #0.1444925
cellStats(lowshrub, sd) #0.07447781

lowshrub.s <- (lowshrub-0.1444925)/0.07447781

# sparseveg
cellStats(sparseveg, mean) #0.1111009
cellStats(sparseveg, sd) #0.08905826

sparseveg.s <- (sparseveg - 0.1111009)/0.08905826


area.raster <- elev 
values(area.raster) <- 1000*1000/10000 # area of a grid pixel, divided by 10000 to standardize

# pull scaled observation/yearlySite covs from dataframe
jul.raster <- elev # create a raster using one thats already in correct crs/scale
values(jul.raster) <- mean(env.s$julian) # replace data with that of what you want - doing it with the mean scaled value of obs covs

wind.raster <- elev 
values(wind.raster) <- mean(env.s$wind)

minsun.raster <- elev 
values(minsun.raster) <- mean(env.s$min_after_sun)

obs.raster <- elev 
values(obs.raster) <- as.factor("DS") # "DS" is the intercept



pred.surface <- stack(tundra.s, lowshrub.s, sparseveg.s, elev.s, area.raster, jul.raster, wind.raster, minsun.raster, obs.raster)

names(pred.surface) <- c("tundra", "lowshrub", "sparseveg", "elev", "area", "julian", "wind", "min_after_sun", "observer")
```

### Poisson Prediction

```{r pois prediction}

p.prediction <- predict(um1.p, type="lambda", newdata=pred.surface)

plot(p.prediction)

#look at some stats
cellStats(p.prediction, "sum") # 642 036
cellStats(p.prediction, "min") # minimum density of 4.9 males/km2
cellStats(p.prediction, "max") # maximum density of 2 917 males/km2
```

### ZIP Prediction

```{r zip prediction}

zip.prediction <- predict(um1.zip, type = "lambda", newdata = pred.surface) 

plot(zip.prediction) 

#look at some stats
cellStats(zip.prediction, "sum") # 3 038 518 mil
cellStats(zip.prediction, "min") # minimum density of 22.9 males/km2
cellStats(zip.prediction, "max") # maximum density of 27 178 males/km2
```
